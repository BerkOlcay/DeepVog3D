{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import tempfile\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "from skimage.io import imread\n",
    "\n",
    "import monai\n",
    "from monai.networks.layers import Norm\n",
    "from monai.data import (\n",
    "    list_data_collate,\n",
    "    ITKReader,\n",
    "    NumpyReader,\n",
    "    decollate_batch\n",
    ")\n",
    "from monai.inferers import SimpleInferer\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    AddChanneld,\n",
    "    RandAdjustContrastd,\n",
    "    Compose,\n",
    "    DivisiblePadd,\n",
    "    DataStatsd,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    Flipd,\n",
    "    Lambdad,\n",
    "    LoadImaged,\n",
    "    RandAdjustContrastd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandRotated,\n",
    "    RandZoomd,\n",
    "    RandFlipd,\n",
    "    RandShiftIntensityd,\n",
    "    RandScaleIntensityd,\n",
    "    RandAffined,\n",
    "    Rand3DElasticd,\n",
    "    RandGaussianNoised,\n",
    "    Resize,\n",
    "    Resized,\n",
    "    ScaleIntensity,\n",
    "    ScaleIntensityd,\n",
    "    SpatialPadd,\n",
    "    ToTensor,\n",
    "    ToTensord,\n",
    "    ToNumpyd,\n",
    "    ToNumpy,\n",
    "    DataStats,\n",
    "    Rotate90d,\n",
    ")\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "import itk\n",
    "\n",
    "# Utility functions\n",
    "def np_sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x)) \n",
    "\n",
    "def arrayStats(arr):\n",
    "    vec = arr.ravel()\n",
    "    stats = {'min':np.min(vec)}\n",
    "    stats['max'] = np.max(vec)\n",
    "    stats['mean'] = np.mean(vec)\n",
    "    stats['perc1'] = np.percentile(vec,1.0)\n",
    "    stats['perc5'] = np.percentile(vec,5.0)\n",
    "    stats['perc25'] = np.percentile(vec,25.0)\n",
    "    stats['perc50'] = np.percentile(vec,50.0)\n",
    "    stats['perc75'] = np.percentile(vec,75.0)\n",
    "    stats['perc95'] = np.percentile(vec,95.0)\n",
    "    stats['perc99'] = np.percentile(vec,99.0)\n",
    "    stats['median'] = np.median(vec)\n",
    "    return stats\n",
    "\n",
    "def draw_segmented_area(frame_rgb, pupil_map_masked, iris_map_masked, glints_map_masked, visible_map_masked, ax=None):\n",
    "    # Plot segmented area\n",
    "    #fig, ax = plt.subplots(figsize=(3.2,2.4))\n",
    "    alpha=0.4\n",
    "    fig_created = False\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(3.2,2.4))\n",
    "        #canvas = FigureCanvas(fig)\n",
    "        ax = fig.subplots()\n",
    "        fig_created = True\n",
    "    ax.imshow(frame_rgb)#, vmax=1, vmin=0, cmap=\"gray\")\n",
    "    ax.imshow(np.ma.masked_where(visible_map_masked<0.5,visible_map_masked), cmap=\"spring\", vmax=1, vmin=0, alpha=alpha)\n",
    "    ax.imshow(np.ma.masked_where(iris_map_masked<0.5,iris_map_masked), cmap=\"GnBu\", vmax=1, vmin=0, alpha=alpha)\n",
    "    ax.imshow(np.ma.masked_where(pupil_map_masked<0.5,pupil_map_masked), cmap=\"OrRd\", vmax=1, vmin=0, alpha=alpha)\n",
    "    ax.imshow(np.ma.masked_where(glints_map_masked<0.5,glints_map_masked), cmap=\"cool\", vmax=1, vmin=0, alpha=alpha)\n",
    "    ax.axis('off')\n",
    "    if fig_created:\n",
    "        fig.tight_layout()\n",
    "        fig.canvas.draw()\n",
    "        plt.show()\n",
    "\n",
    "def draw_segmented_area_4d(frame_rgb, seg_4d, ax=None):\n",
    "    pupil_map_masked, iris_map_masked, glints_map_masked, visible_map_masked = tuple([np.squeeze(seg_4d[:,:,c]) for c in [0,1,2,3]])\n",
    "    draw_segmented_area(frame_rgb, pupil_map_masked, iris_map_masked, glints_map_masked, visible_map_masked, ax=ax)\n",
    "\n",
    "pn_code = 'E:\\\\Users\\\\BerkOlcay\\\\DeepVOG3D\\\\DeepVOG3D\\\\PYTHON'\n",
    "pn_data = 'E:\\\\Users\\\\BerkOlcay\\\\DeepVOG3D\\\\DeepVOG3D\\\\data\\\\data_dv3d_monai_QA'\n",
    "\n",
    "monai.config.print_config()\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "df = pd.read_csv(os.path.join(pn_code,'df_dv3d_monai_files.csv'), index_col=0)\n",
    "\n",
    "# set up dataset splits and dict-lists\n",
    "nr_train_samples = 4000\n",
    "train_idxs = np.arange(nr_train_samples)\n",
    "valid_idxs = np.arange(nr_train_samples,df.shape[0])\n",
    "\n",
    "train_files = [{\"img\": os.path.join(pn_data, fn_img), \"seg\": os.path.join(pn_data, fn_seg)} for fn_img, fn_seg in zip(df.fn_img[train_idxs], df.fn_seg_maps[train_idxs])]\n",
    "val_files = [{\"img\": os.path.join(pn_data, fn_img), \"seg\": os.path.join(pn_data, fn_seg)} for fn_img, fn_seg in zip(df.fn_img[valid_idxs], df.fn_seg_maps[valid_idxs])]\n",
    "\n",
    "print(f'df.columns:\\n {df.columns.tolist()}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Pickle to npy converter\n",
    "for index, item in enumerate(train_files):\n",
    "    with open(item[\"seg\"], 'rb') as file:\n",
    "        arr = pickle.load(file)\n",
    "        np.save(os.path.join(pn_data, item[\"seg\"].replace('.pkl','.npy')), arr)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set deterministic training for reproducibility\n",
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(len(train_files), \"train files\")\n",
    "\n",
    "file_types= []\n",
    "indexes = []\n",
    "for index, item in enumerate(train_files):\n",
    "    extension = os.path.splitext(item['img'])[1]\n",
    "    if (extension not in file_types):\n",
    "        file_types.append(extension)\n",
    "        indexes.append(index)\n",
    "\n",
    "print(\"available formats \", file_types)\n",
    "print(indexes)\n",
    "\n",
    "for i in indexes:\n",
    "    print(train_files[i]['img'])\n",
    "    \n",
    "flag_text_loader_check = True\n",
    "if flag_text_loader_check:\n",
    "    fig, axs = plt.subplots(1,4)\n",
    "    loader = monai.transforms.LoadImage(reader= \"ITKReader\")\n",
    "    for i, index in enumerate(indexes):\n",
    "        img = loader(train_files[index]['img'])\n",
    "        arr=np.array(img[0])\n",
    "        print(arr.shape)\n",
    "        axs[i].imshow(arr/255)\n",
    "        axs[i].set_xlabel(file_types[i])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define own transform: gray2rgb\n",
    "from typing import Any, Dict, Hashable, Mapping, Optional, Sequence, Tuple, Union\n",
    "from monai.config import KeysCollection\n",
    "from monai.transforms.compose import Transform, MapTransform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "class Gray2Rgb(Transform):\n",
    "    \"\"\"\n",
    "    Converts gray image (a single color channel) to RGB (three color channels, identical to channel 0)\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply the transform to `img`.\n",
    "        \"\"\"\n",
    "        if img.ndim == 2:\n",
    "            img = np.expand_dims(img, axis=2)\n",
    "        if img.shape[-1]==1:\n",
    "            img = np.concatenate((img,img,img),axis=2)\n",
    "        elif img.shape[-1]==3:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('Input img to Gray2Rgb needs to have 1 or three channels, but not %d.'%(img.shape[-1]))\n",
    "        return img\n",
    "\n",
    "class Gray2Rgbd(MapTransform):\n",
    "    \"\"\"\n",
    "    Dictionary-based wrapper of :py:class:`Gray2Rgb`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keys: KeysCollection) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            keys: keys of the corresponding items to be transformed.\n",
    "                See also: :py:class:`monai.transforms.compose.MapTransform`\n",
    "            offset: offset value to shift the intensity of image.\n",
    "        \"\"\"\n",
    "        super().__init__(keys)\n",
    "        self.transform = Gray2Rgb()\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, np.ndarray]) -> Dict[Hashable, np.ndarray]:\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            d[key] = self.transform(d[key])\n",
    "        return d\n",
    "\n",
    "class Rgb2Gray(Transform):\n",
    "    \"\"\"\n",
    "    Converts gray image (a single color channel) to RGB (three color channels, identical to channel 0)\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, img: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply the transform to `img`.\n",
    "        \"\"\"\n",
    "        if img.ndim == 2:\n",
    "            pass\n",
    "        elif img.ndim == 3:\n",
    "            assert img.shape[2]==3\n",
    "            img = rgb2gray(img)\n",
    "        else:\n",
    "            raise ValueError('Input img to Rgb2Gray needs to have three channels, but not %d.'%(img.shape[-1]))\n",
    "        return img\n",
    "\n",
    "class Rgb2Grayd(MapTransform):\n",
    "    \"\"\"\n",
    "    Dictionary-based wrapper of :py:class:`Gray2Rgb`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keys: KeysCollection) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            keys: keys of the corresponding items to be transformed.\n",
    "                See also: :py:class:`monai.transforms.compose.MapTransform`\n",
    "            offset: offset value to shift the intensity of image.\n",
    "        \"\"\"\n",
    "        super().__init__(keys)\n",
    "        self.transform = Rgb2Gray()\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, np.ndarray]) -> Dict[Hashable, np.ndarray]:\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            d[key] = self.transform(d[key])\n",
    "        return d\n",
    "\n",
    "'''\n",
    "Here's a nice trick with Lambda function, from Eric Kerfoot, answered in github issue:\n",
    "https://github.com/Project-MONAI/tutorials/issues/65\n",
    "\n",
    "test_transforms = Compose(\n",
    "    [\n",
    "        LoadImage(image_only=True, reader=PILReader()),\n",
    "        Lambda(lambda im: im[...,None] if im.ndim==2 else im),\n",
    "        AsChannelsFirst(),\n",
    "        Resize(crop_size, \"area\"),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "Why does this work:\n",
    "test = np.random.rand(3,3)\n",
    "print(test.shape)\n",
    "test2 = test[...,None]\n",
    "print(test2.shape)\n",
    "\n",
    "'''\n",
    "\n",
    "flag_test_gray2rgb = True\n",
    "if flag_test_gray2rgb:\n",
    "    idx = 1833\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    loader = monai.transforms.LoadImage(reader= \"ITKReader\")\n",
    "    colorizer = Gray2Rgb()\n",
    "    img = loader(train_files[idx]['img'])\n",
    "    arr=np.array(img[0])\n",
    "    print(f'Image shape before Gray2Rgb: {arr.shape}')\n",
    "    axs[0].imshow(arr)\n",
    "    arr=np.array(arr)\n",
    "    arr=colorizer(arr)\n",
    "    print(f'Image shape after Gray2Rgb: {arr.shape}')\n",
    "    plt.imshow(arr/255)\n",
    "    plt.show()\n",
    "    \n",
    "flag_test_rgb2gray = True\n",
    "if flag_test_rgb2gray:\n",
    "    idx = 0 #1834\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    loader = monai.transforms.LoadImage(reader= \"ITKReader\")\n",
    "    decolorizer = Rgb2Gray()\n",
    "    img = loader(train_files[idx]['img'])\n",
    "    arr=np.array(img[0])\n",
    "    print(f'Image shape before Rgb2Gray: {arr.shape}')\n",
    "    axs[0].imshow(arr/255)\n",
    "    arr=decolorizer(arr)\n",
    "    print(f'Image shape after Rgb2Gray: {arr.shape}')\n",
    "    axs[1].imshow(arr/255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transforms for image and segmentation\n",
    "img_size = np.array([240,320])\n",
    "rot_max = 45*np.pi/180.0\n",
    "shear_max = 0.5\n",
    "trans_max = tuple((img_size*0.15).astype(int))\n",
    "scale_max = 0.25\n",
    "'''\n",
    "further ideas for augmentations\n",
    "- Random colorization \n",
    "    * Uniform hue, contrast, hist.eq., white balance, sharpen\n",
    "    * Torchvision ColorJitter (https://pytorch.org/docs/stable/_modules/torchvision/transforms/transforms.html#ColorJitter)\n",
    "- Gaussian noise!\n",
    "    * see pupillometry video, at 250 Hz, super noisy, vertical banding pattern too\n",
    "'''\n",
    "\n",
    "def gray2rgb(x):\n",
    "    #print(x.shape)\n",
    "    if x.shape[0]==1:\n",
    "        x = x.repeat(3, 1, 1)\n",
    "        x.meta['original_channel_dim'] = -1 # THIS is the important line! \n",
    "    #print(x.shape)\n",
    "    return x\n",
    "\n",
    "def clean_tiff_meta(x):\n",
    "    for key in ['DocumentName', 'ImageDescription', 'Software']:\n",
    "        if key in x.meta.keys():\n",
    "            del x.meta[key]\n",
    "    return x\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        #Lambdad(keys=['img', 'seg'], func=lambda x: print(x), overwrite = False),\n",
    "        LoadImaged(keys=[\"img\"], reader= ITKReader, image_only = True),\n",
    "        LoadImaged(keys=[\"seg\"], reader=NumpyReader, image_only = True),\n",
    "        EnsureChannelFirstd(keys=[\"img\"]),\n",
    "        Lambdad(keys=['img'], func=lambda x: gray2rgb(x)), # gray to rgb conversion\n",
    "        ScaleIntensityd(keys=\"img\"),        \n",
    "        Flipd(keys=[\"seg\"], spatial_axis=[1]), # necessary due to various readers ITKReader and NumpyReader\n",
    "        Rotate90d(keys=[\"seg\"]), # necessary due to various readers ITKReader and NumpyReader\n",
    "        Rotate90d(keys=[\"img\", \"seg\"]), # necessary due to various readers ITKReader and NumpyReader\n",
    "        Resized(keys=[\"img\", \"seg\"], spatial_size=(240,320)),\n",
    "        RandAdjustContrastd(keys=[\"seg\"], prob=1.0, gamma=(0.1, 10.0)),\n",
    "        RandFlipd(keys=[\"img\", \"seg\"], prob=0.5, spatial_axis=[0,1]),\n",
    "        RandAffined(keys=[\"img\", \"seg\"], prob=0.5,\n",
    "                    rotate_range=rot_max, \n",
    "                    shear_range=shear_max, \n",
    "                    translate_range=trans_max, \n",
    "                    padding_mode='zeros'),\n",
    "        EnsureTyped(keys=\"img\"),\n",
    "        Lambdad(keys=['img'], func=lambda x: clean_tiff_meta(x)), # clean weird keys in TIFF metadata - turns out this is not necessary\n",
    "        ToTensord(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "#                     scale_range=scale_max, \n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        #Lambdad(keys=['img', 'seg'], func=lambda x: print(x), overwrite = False),\n",
    "        LoadImaged(keys=[\"img\"], reader= ITKReader, image_only = True),\n",
    "        LoadImaged(keys=[\"seg\"], reader=NumpyReader, image_only = True),\n",
    "        #DataStatsd(keys=[\"img\", \"seg\"]),\n",
    "        EnsureChannelFirstd(keys=[\"img\"]),\n",
    "        Lambdad(keys=['img'], func=lambda x: gray2rgb(x)), # gray to rgb conversion\n",
    "        ScaleIntensityd(keys=\"img\"),\n",
    "        Flipd(keys=[\"seg\"], spatial_axis=[1]), # necessary due to various readers ITKReader and NumpyReader\n",
    "        Rotate90d(keys=[\"seg\"]), # necessary due to various readers ITKReader and NumpyReader\n",
    "        Rotate90d(keys=[\"img\", \"seg\"]), # necessary due to various readers ITKReader and NumpyReader\n",
    "        Resized(keys=[\"img\", \"seg\"], spatial_size=(240,320)),\n",
    "        EnsureTyped(keys=\"img\"),\n",
    "        Lambdad(keys=['img'], func=lambda x: clean_tiff_meta(x)), # clean weird keys in TIFF metadata - turns out this is not necessary\n",
    "        ToTensord(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "#         Rgb2Grayd(keys=[\"img\"]),\n",
    "#         ToNumpyd(keys=[\"img\"]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Pickle to npy converter\n",
    "for index, item in enumerate(val_files):\n",
    "    with open( item[\"seg\"], 'rb') as file:\n",
    "        arr = pickle.load(file)\n",
    "        print(\"item \", index, item[\"seg\"], \"loaded.\")        \n",
    "\n",
    "    ylabel = np.array([])\n",
    "    #print (arr)\n",
    "    #print (\"arr.shape \", arr.shape)\n",
    "\n",
    "    for key, value in arr.items() :\n",
    "        if (key != \"useful\"):\n",
    "            #print (key, \"\\n\", value)\n",
    "            #print (\"val \", value.sum())\n",
    "            #print (\"len \", len(value))\n",
    "            #print (\"len(value[0]) \", len(value[0]))\n",
    "            tmp_arr = np.asarray(value)\n",
    "            tmp_arr = tmp_arr[np.newaxis]\n",
    "            #print(\"tmp_arr.shape \", tmp_arr.shape)\n",
    "            if (ylabel.size == 0):\n",
    "                ylabel = tmp_arr\n",
    "            else:\n",
    "                ylabel = np.append(ylabel, tmp_arr, axis=0)\n",
    "            #print (\"ylabel.shape in loop \", ylabel.shape)        \n",
    "\n",
    "    print (\"ylabel.shape \", ylabel.shape)\n",
    "\n",
    "    save = True\n",
    "    if (save):\n",
    "        fileObject = open(item[\"seg\"], 'wb')\n",
    "        pickle.dump(ylabel, fileObject)\n",
    "        fileObject.close()\n",
    "        print(item[\"seg\"], \" updated.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_empty_labels():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    check_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "    train_loader = DataLoader(check_ds, batch_size=1, num_workers=0)\n",
    "    empty_labels_train_indexes = []\n",
    "    print (\"iterating training data\")\n",
    "    for index, batch_data in enumerate(train_loader):\n",
    "        train_labels = batch_data[\"seg\"].to(device)\n",
    "        if (train_labels.max() == 0.):\n",
    "            empty_labels_train_indexes.append(index)\n",
    "        if (index%(len(train_files)/5) == 0 and index != 0):\n",
    "            print(\"%\", index/len(train_files) *100, \" completed\")\n",
    "    val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n",
    "    val_loader = DataLoader(val_ds, batch_size=1, num_workers=0, collate_fn=list_data_collate)\n",
    "    empty_labels_validation_indexes = []\n",
    "    print (\"iterating validation data\")\n",
    "    for index, batch_data in enumerate(val_loader):\n",
    "        val_labels = batch_data[\"seg\"].to(device)\n",
    "        if (val_labels.max() == 0.):\n",
    "            empty_labels_validation_indexes.append(index)\n",
    "        if (index%(len(val_files)/5) == 0 and index != 0):\n",
    "            print(\"%\", index/len(val_files) * 100, \" completed\")\n",
    "        \n",
    "    return empty_labels_train_indexes, empty_labels_validation_indexes\n",
    "    \n",
    "empty_labels_train_indexes, empty_labels_validation_indexes = check_empty_labels()\n",
    "\n",
    "print (\"indexes of not labeled training data: (\", len(empty_labels_train_indexes), \"images )\")\n",
    "print(empty_labels_train_indexes)\n",
    "for index in empty_labels_train_indexes:\n",
    "    print (index, train_files[index][\"img\"])\n",
    "\n",
    "print (\"indexes of not labeled training data: (\", len(empty_labels_validation_indexes), \"images )\")\n",
    "print(empty_labels_validation_indexes)\n",
    "for index in empty_labels_validation_indexes:\n",
    "    print (index, val_files[index][\"img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train files length before\", len(train_files))\n",
    "print(\"validation files length before\", len(val_files))\n",
    "\n",
    "for i in sorted(empty_labels_train_indexes, reverse=True):\n",
    "    del train_files[i]\n",
    "for i in sorted(empty_labels_validation_indexes, reverse=True):\n",
    "    del val_files[i]\n",
    "    \n",
    "print(\"train files length after\", len(train_files))\n",
    "print(\"validation files length after\", len(val_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\"]),\n",
    "        LoadImaged(keys=[\"seg\"], reader=NumpyReader),\n",
    "        Gray2Rgbd(keys=[\"img\"]),\n",
    "        AsChannelFirstd(keys=[\"img\"]),\n",
    "        ScaleIntensityd(keys=\"img\"),\n",
    "        RandFlipd(keys=[\"seg\"], prob=1, spatial_axis=[1]),\n",
    "        Rotate90d(keys=[\"seg\"]),\n",
    "        Resized(keys=[\"img\", \"seg\"], spatial_size=(240,320)),\n",
    "        RandAdjustContrastd(keys=[\"seg\"], prob=1.0, gamma=(0.1, 10.0)),\n",
    "        RandFlipd(keys=[\"img\", \"seg\"], prob=0.5, spatial_axis=[0,1]),\n",
    "        RandAffined(keys=[\"img\", \"seg\"], prob=0.5,\n",
    "                    rotate_range=rot_max, \n",
    "                    shear_range=shear_max, \n",
    "                    translate_range=trans_max, \n",
    "                    padding_mode='zeros'),\n",
    "        ToTensord(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "'''\n",
    "npc = ToNumpy()\n",
    "check_ds = monai.data.Dataset(data=train_files[1000:], transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=10, num_workers=0)\n",
    "check_data = first(check_loader)\n",
    "for i in range(1):\n",
    "    # use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n",
    "    # channel first versions\n",
    "    img_cf = np.squeeze(npc(check_data[\"img\"])[i,:,:,:])\n",
    "    seg_cf = np.squeeze(npc(check_data[\"seg\"])[i,:,:,:])\n",
    "    #print(\"npc(check_data[seg] \", npc(check_data[\"seg\"]).shape)\n",
    "    #print(\"seg_cf.shape\", seg_cf.shape)\n",
    "    #print(\"seg_cf.shape\", seg_cf.shape)\n",
    "    #print (seg_cf)\n",
    "    for i in range(4):\n",
    "        print (seg_cf[i,:,:].sum())\n",
    "    # channel last versions for plotting\n",
    "    pupil_map_masked, iris_map_masked, visible_map_masked, glints_map_masked, = tuple([np.squeeze(seg_cf[c,:,:]) for c in [0,1,2,3]])\n",
    "    img = np.moveaxis(img_cf, [0,1,2], [-1,-3,-2])\n",
    "    draw_segmented_area(img, pupil_map_masked, iris_map_masked, glints_map_masked, visible_map_masked)\n",
    "    #plt.imshow(img)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check loaded images and augmentations\n",
    "# define check dataset, check data loader\n",
    "npc = ToNumpy()\n",
    "check_batch_size = 10\n",
    "check_ds = monai.data.Dataset(data=train_files[1000:], transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=check_batch_size, num_workers=0)\n",
    "check_data = first(check_loader) #check_data = monai.utils.misc.first(check_loader)\n",
    "for i in range(check_batch_size):\n",
    "    # use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n",
    "    # channel first versions\n",
    "    img_cf = np.squeeze(npc(check_data[\"img\"])[i,:,:,:])\n",
    "    seg_cf = np.squeeze(npc(check_data[\"seg\"])[i,:,:,:])\n",
    "    # channel last versions for plotting\n",
    "    pupil_map_masked, iris_map_masked, visible_map_masked, glints_map_masked = tuple([np.squeeze(seg_cf[c,:,:]) for c in [0,1,2,3]])\n",
    "    img = np.moveaxis(img_cf, [0,1,2], [-1,-3,-2])\n",
    "    draw_segmented_area(img, pupil_map_masked, iris_map_masked, glints_map_masked, visible_map_masked)\n",
    "    #plt.imshow(img)\n",
    "    #plt.show()\n",
    "\n",
    "# check filenames in minibatch\n",
    "# check_data['img_meta_dict']['format']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(len(train_files))\n",
    "        \n",
    "# remove elements from list that contain given string \n",
    "# jpg files violate document name because of .mp4 in the name\n",
    "train_files = [item for item in train_files if \".mp4\" not in item['img']]\n",
    "print(\"after jpgs \", len(train_files))\n",
    "# tiff files procude error when you want to print batch_data['img_meta_dict']['format']\n",
    "train_files = [item for item in train_files if \".tiff\" not in item['img']]\n",
    "print(\"after tiffs \", len(train_files))\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create cached data loaders for training and validation\n",
    "t0 = time.time()\n",
    "train_ds = monai.data.CacheDataset(data=train_files, transform=train_transforms)\n",
    "# use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "#    num_workers=6,\n",
    "    collate_fn=list_data_collate,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "# create a validation data loader\n",
    "val_ds = monai.data.CacheDataset(data=val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=1, \n",
    "    num_workers=0, \n",
    "#    num_workers=4,\n",
    "    collate_fn=list_data_collate\n",
    ")\n",
    "#dice_metric = DiceMetric(include_background=True, to_onehot_y=False, sigmoid=True, reduction=\"mean\")\n",
    "t1 = time.time()\n",
    "print('Elapsed time: %0.2f sec.'%(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_epochs = 300\n",
    "\n",
    "# create UNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "model = monai.networks.nets.UNet(\n",
    "    dimensions=2,\n",
    "    in_channels=3,\n",
    "    out_channels=5,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    dropout=0.2,\n",
    "    #dropout=0.5,\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "loss_function = monai.losses.DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, softmax=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose(\n",
    "    [Activations(softmax=True), AsDiscrete(threshold=0.5)]\n",
    ")\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "inferer = SimpleInferer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_interval = 1\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_pupil = []\n",
    "metric_values_iris = []\n",
    "metric_values_visible = []\n",
    "metric_values_glints = []\n",
    "metric_values_irrelevant = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"img\"].to(device),\n",
    "            batch_data[\"seg\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        '''\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "        '''\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"img\"].to(device),\n",
    "                    val_data[\"seg\"].to(device),\n",
    "                )\n",
    "                val_outputs = inferer(val_inputs, model)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            \n",
    "            metric_pupil = metric_batch[0].item()\n",
    "            metric_values_pupil.append(metric_pupil)\n",
    "            metric_iris = metric_batch[1].item()\n",
    "            metric_values_iris.append(metric_iris)\n",
    "            metric_visible = metric_batch[2].item()\n",
    "            metric_values_visible.append(metric_visible)\n",
    "            metric_glints = metric_batch[3].item()\n",
    "            metric_values_glints.append(metric_glints)\n",
    "            metric_irrelevant = metric_batch[4].item()\n",
    "            metric_values_irrelevant.append(metric_irrelevant)\n",
    "            \n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                torch.save(model.state_dict(), \"best_metric_model_dv3d_segmentation2d_dict_withdropout.pth\")\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" pupil: {metric_pupil:.4f} iris: {metric_iris:.4f} visible: {metric_visible:.4f} glints: {metric_glints:.4f} irrelevant: {metric_irrelevant:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at test results\n",
    "val_loader_test = DataLoader(val_ds, batch_size=1, num_workers=0, collate_fn=list_data_collate)\n",
    "ff_model_weights = 'best_metric_model_dv3d_segmentation2d_dict_withdropout.pth'\n",
    "model.load_state_dict(torch.load(ff_model_weights))\n",
    "model.eval()\n",
    "\n",
    "list_imgs  = []\n",
    "list_segs  = []\n",
    "list_preds = []\n",
    "with torch.no_grad():\n",
    "    metric_sum = 0.0\n",
    "    metric_count = 0\n",
    "    val_images = None\n",
    "    val_labels = None\n",
    "    val_outputs = None\n",
    "    tgt_idx = 5\n",
    "    for val_data in tqdm(val_loader_test): #val_loader\n",
    "        val_images, val_labels = val_data[\"img\"].to(device), val_data[\"seg\"].to(device)\n",
    "        val_outputs = inferer(val_images, model)\n",
    "        list_imgs.append(val_images)\n",
    "        list_segs.append(val_labels)\n",
    "        list_preds.append(val_outputs)\n",
    "        #value = dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "        #metric_count += len(value)\n",
    "        #metric_sum += value.item() * len(value)\n",
    "    #metric = metric_sum / metric_count\n",
    "    #metric_values.append(metric)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sig_act = torch.nn.Sigmoid()\n",
    "sof_act = torch.nn.Softmax()\n",
    "for idx in range(100,200):\n",
    "    img  = np.squeeze(list_imgs[idx].cpu().numpy()).transpose([1,2,0])\n",
    "    seg  = np.squeeze(list_segs[idx].cpu().numpy()).transpose([1,2,0])\n",
    "    pred = np.squeeze(sof_act(list_preds[idx]).cpu().numpy()).transpose([1,2,0])\n",
    "    \n",
    "    print('Image index: %d'%idx)\n",
    "    fig,axs = plt.subplots(1,3,figsize=(18,6))\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].axis('off')\n",
    "    # seg\n",
    "    pupil_map_masked, iris_map_masked, visible_map_masked, glints_map_masked = tuple([np.squeeze(seg[:,:,c]) for c in [0,1,2,3]])\n",
    "    draw_segmented_area(img, pupil_map_masked, iris_map_masked, glints_map_masked, visible_map_masked,ax=axs[1])\n",
    "    pupil_map_masked, iris_map_masked, visible_map_masked, glints_map_masked = tuple([np.squeeze(pred[:,:,c]) for c in [0,1,2,3]])\n",
    "    #draw_segmented_area(img, pupil_map_masked, iris_map_masked, glints_map_masked, tmp, ax=axs[2])\n",
    "    draw_segmented_area(img, pupil_map_masked, iris_map_masked, glints_map_masked, visible_map_masked, ax=axs[2])\n",
    "    #img = np.moveaxis(img_cf, [0,1,2], [-1,-3,-2])\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try inferring from a skimage loaded numpy array\n",
    "g2r = Gray2Rgb()\n",
    "acf = AsChannelFirst()\n",
    "acl = AsChannelLast()\n",
    "res = Resize(spatial_size=(240,320))\n",
    "pred_transforms = Compose(\n",
    "    [\n",
    "        Gray2Rgb(),\n",
    "        AsChannelFirst(),\n",
    "        ScaleIntensity(),\n",
    "        Resize(spatial_size=(240,320)),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")\n",
    "img_proc = pred_transforms(img)\n",
    "#plt.imshow(acl(res(acf(img)))/255); plt.show()\n",
    "\n",
    "seg_out = model(img_proc.unsqueeze(0).to(device)).detach().cpu().numpy()\n",
    "seg_out = acl(np.squeeze(seg_out))\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].imshow(img)\n",
    "draw_segmented_area_4d(acl(res(acf(img)))/255, seg_out, ax=axs[1])\n",
    "axs[1].axis('on')\n",
    "plt.show()\n",
    "\n",
    "# now, try for an image stack\n",
    "img = imread(train_files[0]['img'])\n",
    "print(img.shape)\n",
    "imgs = np.stack(10*[img],axis=0)\n",
    "imgs_proc = []\n",
    "for I in imgs:\n",
    "    img_proc = pred_transforms(I)\n",
    "    imgs_proc.append(img_proc)\n",
    "imgs_proc = torch.stack(imgs_proc)\n",
    "print(imgs_proc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = img[None, ...]\n",
    "print(img.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supplemental steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Fixing a lot of shit...\n",
    "print('PNG rgb')\n",
    "ff = train_files[0]['img']\n",
    "print(ff)\n",
    "img = Image.open(ff)\n",
    "print(img.size)\n",
    "print(np.asarray(img).shape)\n",
    "print(dir(img))\n",
    "print(img.__class__)\n",
    "print(img.info)\n",
    "print('\\n\\n')\n",
    "\n",
    "print('PNG gray')\n",
    "ff = train_files[1024]['img']\n",
    "print(ff)\n",
    "img = Image.open(ff)\n",
    "print(img.size)\n",
    "print(np.asarray(img).shape)\n",
    "print(dir(img))\n",
    "print(img.__class__)\n",
    "print(img.info)\n",
    "print('\\n\\n')\n",
    "\n",
    "print('JPG')\n",
    "ff = train_files[2723]['img']\n",
    "print(ff)\n",
    "img = Image.open(ff)\n",
    "print(img.size)\n",
    "print(np.asarray(img).shape)\n",
    "print(dir(img))\n",
    "print(img.__class__)\n",
    "print(img.info)\n",
    "print('\\n\\n')\n",
    "\n",
    "print('BMP')\n",
    "ff = train_files[2600]['img']\n",
    "print(ff)\n",
    "img = Image.open(ff)\n",
    "print(img.size)\n",
    "print(np.asarray(img).shape)\n",
    "print(dir(img))\n",
    "print(img.__class__)\n",
    "print(img.info)\n",
    "print('\\n\\n')\n",
    "\n",
    "#print(img.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data['img_meta_dict']['filename_or_obj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ff in batch_data['img_meta_dict']['filename_or_obj']:\n",
    "    img = Image.open(ff)\n",
    "    print(f'{img.info}\\t: {ff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test whether we can get rid of the 'dpi' key in img.info \n",
    "ff = '/data/mangotee/Projects/DeepVOG/data/dv3d_monai/data_dv3d_monai_QA/10262_mmuiris_subject028_ongbll3_000000.png'\n",
    "#print(ff)\n",
    "shutil.copy(ff,os.getcwd())\n",
    "img = Image.open(ff_in)\n",
    "if 'dpi' in img.info.keys():\n",
    "    print('Has dpi!')\n",
    "\n",
    "ff_in = '/data/mangotee/Projects/IEVnet/PYTHON/10262_mmuiris_subject028_ongbll3_000000.png'\n",
    "ff_out = '/data/mangotee/Projects/IEVnet/PYTHON/10262_mmuiris_subject028_ongbll3_000000_corr.png'\n",
    "img = Image.open(ff_in)\n",
    "print(img.info)\n",
    "img.info={}\n",
    "img.save(ff_out)\n",
    "img_corr = Image.open(ff_out)\n",
    "print(img_corr.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load seg_maps, stack into 3D array with 4 channels, write out as npy files, to use with monai.transforms.LoadNumpyd\n",
    "# has already been done once\n",
    "'''\n",
    "Format from deepvog3D:\n",
    "0/1/2/3 ... pupil_map/iris_map/combined_glints_map/visible_map\n",
    "    img_shape = (pred.shape[0], pred.shape[1])\n",
    "    useful_map = np.zeros(img_shape)\n",
    "    pupil_map = pred[:,:,0]\n",
    "    iris_map = pred[:,:,1]\n",
    "    combined_glints_map = pred[:,:,2]\n",
    "    visible_map = pred[:,:,3]\n",
    "    useful_map[(pupil_map == 0) & (iris_map == 1) & (visible_map == 1) & (combined_glints_map == 0)] = 1\n",
    "    return useful_map, (pupil_map, iris_map, combined_glints_map, visible_map)\n",
    "'''\n",
    "if False:\n",
    "    # has to be done only once\n",
    "    df = pd.read_csv(os.path.join(pn_data,'..','df_dv3d_monai_files.csv'), index_col=0)\n",
    "    df['fn_seg_maps_np'] = [s.replace('.pkl','.npy') for s in df.fn_seg_maps]\n",
    "    # \n",
    "    print(df.columns)\n",
    "\n",
    "    for idx, ff_np_out in enumerate(tqdm(df.fn_seg_maps_np)):\n",
    "        with open(os.path.join(pn_data,df.fn_seg_maps[idx]), 'rb') as file:\n",
    "            seg_maps = pickle.load(file)\n",
    "        seg_maps_np = np.zeros(seg_maps['pupil'].shape+(4,)).astype(np.bool)\n",
    "        seg_maps_np[:,:,0] = seg_maps['pupil']\n",
    "        seg_maps_np[:,:,1] = seg_maps['iris']\n",
    "        seg_maps_np[:,:,2] = seg_maps['glints']\n",
    "        seg_maps_np[:,:,3] = seg_maps['visible']\n",
    "        np.save(ff_np_out, seg_maps_np)\n",
    "\n",
    "# there is another fix we need to do - replace TIF files with PNG files\n",
    "if False:\n",
    "    from skimage.io import imread, imsave\n",
    "    fns_png = []\n",
    "    for idx, fn_img in enumerate(tqdm(df.fn_img)):\n",
    "        if os.path.splitext(fn_img)[1]=='.tiff':\n",
    "            img = imread(os.path.join(pn_data, fn_img))\n",
    "            fn_png = fn_img.replace('.tiff','.png')\n",
    "            imsave(os.path.join(pn_data, fn_png),img)\n",
    "            fns_png.append(fn_png)\n",
    "        else:\n",
    "            fns_png.append(fn_img)\n",
    "    df['fn_img_with_tiff'] = df.fn_img.tolist()\n",
    "    df['fn_img'] = fns_png\n",
    "\n",
    "# aaand another fix we need to do - replace BMP files with PNG files\n",
    "if False:\n",
    "    from skimage.io import imread, imsave\n",
    "    fns_png = []\n",
    "    for idx, fn_img in enumerate(tqdm(df.fn_img)):\n",
    "        ext = os.path.splitext(fn_img)[1]\n",
    "        if ext=='.bmp' or ext=='.jpg':\n",
    "            img = imread(os.path.join(pn_data, fn_img))\n",
    "            fn_png = fn_img.replace(ext,'.png')\n",
    "            imsave(os.path.join(pn_data, fn_png),img)\n",
    "            fns_png.append(fn_png)\n",
    "        else:\n",
    "            fns_png.append(fn_img)\n",
    "    df['fn_img_with_bmp_jpg'] = df.fn_img.tolist()\n",
    "    df['fn_img'] = fns_png\n",
    "    # takes 2 mins\n",
    "\n",
    "# that didn't do it either... now... we need to load each image, erase the dict in info, and save back to png\n",
    "# the actual problem was that some of the images (mmuiris and delhi) had a 'dpi' key in their img.info dict\n",
    "# three days lost...\n",
    "if True:\n",
    "    counter = 0\n",
    "    for idx, fn_img in enumerate(tqdm(df.fn_img)):\n",
    "        ff = os.path.join(pn_data, fn_img)\n",
    "        img = Image.open(ff)\n",
    "        if 'dpi' in img.info.keys():\n",
    "            print(f'Has dpi! {ff}')\n",
    "            counter +=1\n",
    "        img.info={}\n",
    "        img.save(ff)\n",
    "    print('Fixed %d files with dpi tag.'%counter)\n",
    "    # Fixed 952 files with dpi tag.\n",
    "    # Took ~5 mins\n",
    "\n",
    "# save the df\n",
    "# df.to_csv(os.path.join(pn_data,'..','df_dv3d_monai_files.csv'))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Demo Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_files[1][\"img\"] = \"E:\\\\Users\\\\BerkOlcay\\\\DeepVOG3D\\\\DeepVOG3D\\\\PYTHON\\\\binkoularRight.png\"\n",
    "val_files[0][\"img\"] = \"E:\\\\Users\\\\BerkOlcay\\\\DeepVOG3D\\\\DeepVOG3D\\\\PYTHON\\\\binkoularLeft.png\"\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "loader = monai.transforms.LoadImage(reader= \"ITKReader\")\n",
    "for i in range(2):\n",
    "    img = loader(val_files[i]['img'])\n",
    "    arr=np.array(img[0])\n",
    "    arr = arr[:,:,:3]\n",
    "    print(arr.shape)\n",
    "    axs[i].imshow(arr/255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# look at test results\n",
    "val_ds = monai.data.Dataset(data=val_files[:2], transform=val_transforms)\n",
    "val_loader_test = DataLoader(val_ds, batch_size=1, num_workers=0, collate_fn=list_data_collate)\n",
    "\n",
    "ff_model_weights = 'best_metric_model_dv3d_segmentation2d_dict_withdropout.pth'\n",
    "model.load_state_dict(torch.load(ff_model_weights))\n",
    "model.eval()\n",
    "\n",
    "list_imgs  = []\n",
    "list_preds = []\n",
    "with torch.no_grad():\n",
    "    metric_sum = 0.0\n",
    "    metric_count = 0\n",
    "    val_images = None\n",
    "    val_labels = None\n",
    "    val_outputs = None\n",
    "    tgt_idx = 5\n",
    "    for val_data in tqdm(val_loader_test): #val_loader\n",
    "        val_images, val_labels = val_data[\"img\"].to(device), val_data[\"seg\"].to(device)\n",
    "        val_images = val_images[:,:3,:,:]\n",
    "        val_outputs = inferer(val_images, model)\n",
    "        list_imgs.append(val_images)\n",
    "        list_preds.append(val_outputs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sig_act = torch.nn.Sigmoid()\n",
    "img1  = np.squeeze(list_imgs[0].cpu().numpy()).transpose([1,2,0])\n",
    "pred1 = np.squeeze(sof_act(list_preds[0]).cpu().numpy()).transpose([1,2,0])\n",
    "img1 = np.flipud(img1)\n",
    "pred1 = np.flipud(pred1)\n",
    "img2  = np.squeeze(list_imgs[1].cpu().numpy()).transpose([1,2,0])\n",
    "pred2 = np.squeeze(sof_act(list_preds[1]).cpu().numpy()).transpose([1,2,0])\n",
    "img2 = np.flipud(img2)\n",
    "pred2 = np.flipud(pred2)\n",
    "\n",
    "\n",
    "fig,axs = plt.subplots(1,2,figsize=(12,9))\n",
    "axs[0].imshow(img1)\n",
    "axs[0].axis('off')\n",
    "axs[1].imshow(img2)\n",
    "axs[1].axis('off')\n",
    "plt.show()\n",
    "\n",
    "# seg\n",
    "fig,axs = plt.subplots(1,2,figsize=(12,9))\n",
    "pupil_map_masked, iris_map_masked, visible_map_masked, glints_map_masked = tuple([np.squeeze(pred1[:,:,c]) for c in [0,1,2,3]])\n",
    "draw_segmented_area(img1, pupil_map_masked, iris_map_masked, glints_map_masked, visible_map_masked,ax=axs[0])\n",
    "pupil_map_masked, iris_map_masked, visible_map_masked, glints_map_masked = tuple([np.squeeze(pred2[:,:,c]) for c in [0,1,2,3]])\n",
    "draw_segmented_area(img2, pupil_map_masked, iris_map_masked, glints_map_masked, visible_map_masked,ax=axs[1])\n",
    "plt.show()\n",
    "fig.savefig('segmentation.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
